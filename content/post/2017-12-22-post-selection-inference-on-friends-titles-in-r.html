---
title: "Post-selection inference on Friends titles in R"
author: "Jesse Tweedle"
date: 2017-12-22T15:33:11-05:00
description: "This post applies post-selection procedure to estimating the effect of character names in Friends episode titles on their ratings. Ross is good, apparently."
tags: ["r", "friends", "tidytext", "glmnet", "lasso", "post-selection inference", "econometrics"]
categories: ["r"]
---



<div id="goal" class="section level2">
<h2>Goal</h2>
<p>I want to be a Friends scriptwriter. Can I pick a title that makes an episode an automatic classic? If I just include a character‚Äôs name in the title, does it make it automatically popular? I assume I should just write ‚ÄúThe One Where Rachel is Rachel‚Äù. But let‚Äôs investigate.</p>
<p>The question: what is the effect of a character‚Äôs name in the episode title on the episode rating?</p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>We‚Äôll use Friends data we <code>rvest</code>ed from IMDB in a <a href="/post/2017-12-18-rvest-friends-episodes/">previous post</a>, or just load it via <code>devtools::install_github('tweed1e/werfriends')</code>.</p>
<pre class="r"><code>titles &lt;- werfriends::friends_episodes %&gt;% as_tibble()
titles
## # A tibble: 236 x 7
##    season episode                                           title rating
##     &lt;dbl&gt;   &lt;dbl&gt;                                           &lt;chr&gt;  &lt;dbl&gt;
##  1      1       1           The One Where Monica Gets a Roommate¬†    8.5
##  2      1       2           The One with the Sonogram at the End¬†    8.2
##  3      1       3                         The One with the Thumb¬†    8.3
##  4      1       4             The One with George Stephanopoulos¬†    8.3
##  5      1       5 The One with the East German Laundry Detergent¬†    8.6
##  6      1       6                          The One with the Butt¬†    8.3
##  7      1       7                      The One with the Blackout¬†    9.0
##  8      1       8                  The One Where Nana Dies Twice¬†    8.2
##  9      1       9               The One Where Underdog Gets Away¬†    8.3
## 10      1      10                        The One with the Monkey¬†    8.2
## # ... with 226 more rows, and 3 more variables: n_ratings &lt;dbl&gt;,
## #   director &lt;list&gt;, writers &lt;list&gt;</code></pre>
<p>The problem: all we really have is titles, writers and directors. How do we use natural language information to predict episode ratings? My first idea is to use dummy variables (aka factors with levels <code>c(&quot;0&quot;,&quot;1&quot;)</code>) to encode the title words and director and writer names. In other words, the S4E19 episode titled ‚ÄúThe One with All the Haste‚Äù would have a <code>1</code> in variable <code>haste</code>, but a <code>0</code> for every character dummy variable <code>ross</code>, <code>rachel</code>, etc. (S4E19 is the ep where Monica and Rachel steal their apartment back from Chandler and Joey. So good.)</p>
</div>
<div id="method" class="section level2">
<h2>Method</h2>
<p>So, once we create language dummy variables to try to control for episode characteristics that may affect ratings, we find ourselves with too many predictors. There are only 236 episodes but we create way more language dummies ü§î.</p>
<p>This is a high-dimensional problem, with more explanatory variables than observations, <code>p&gt;&gt;n</code>. A few econometricians have proposed a post-selection estimator to solve these problems, which have a slightly different flavour than normal machine learning / prediction problems.</p>
<p>Consider the goal: I want to know the effect of including a character‚Äôs name in an episode title. LASSO selects important variables that predict the outcome (rating, in my case). If we don‚Äôt penalize the character names, and there is another variable that is correlated with the outcome and one of the character names, then we end up with a biased estimate of the character effect. E.g., say most ‚ÄúRoss‚Äù episodes are directed by Kevin Bright, who is a ‚Äúgood‚Äù director according to the mean rating of all the episodes he directed). Then LASSO selects ‚ÄúRoss‚Äù to be in the model, but drops ‚ÄúKevin Bright‚Äù because it‚Äôs so correlated with ‚ÄúRoss‚Äù (which means it doesn‚Äôt improve the rating prediction much), and LASSO will tell us that ‚ÄúRoss‚Äù has a high positive coefficient. But we know that Ross is trash and it‚Äôs only because Kevin Bright directed his episodes!</p>
<p>The solution is called <strong>post-selection inference</strong>, explained in <a href="http://www.jstor.org/stable/23723483?casa_token=UtP_wvb6MYcAAAAA:jiY5Fzx0lZppTE4J3hiXcZu4_-_zxqjz1n9cc5dsQELc8NnEE8msIi3ZirYaGAXJLR1Dns_QV0pWbAqvneNm-U4OPuNNDWAzBqFjglKnX4Mg_RwyKys6kA&amp;seq=9#page_scan_tab_contents">‚ÄúHigh-dimensional methods and inference on structural and treatment effects‚Äù</a> (Belloni, et al. 2013). They actually call it a ‚Äòdouble selection procedure‚Äô or post-double-selection, but post-selection is fine with me.</p>
<p>The idea is: run LASSO on the outcome variable and everything but the character names; then run LASSO on each of the character names and the rest of the RHS variables. Take all the non-zero coefficients from all of these LASSO results, then regress rating on all the selected variables via a linear model (aka ordinary least squares).</p>
</div>
<div id="code" class="section level2">
<h2>Code</h2>
<p>There are a few annoying steps.</p>
<p>First, I use <code>tidytext</code> to convert titles to words, and then <code>spread</code> those words into dummy factors for the dataset (the dataset is very ‚Äòwide‚Äô after this). I create dummy factors for directors and writers as well.</p>
<p>Next, run the naive linear model, and save the estimates. Then run naive LASSO and save the estimates. Then run the double-selection procedure.</p>
<div id="process" class="section level3">
<h3>Process</h3>
<p>Do ugly stuff (<a href="https://github.com/tweed1e/weblog/tree/master/content/post/2017-12-22-post-selection-inference-on-friends-titles-in-r">check source</a>) for full code and end up with this:</p>
<pre class="r"><code># the dataset
episodes
## # A tibble: 236 x 352
##    rating chandler   joey monica phoebe rachel   ross season episode
##     &lt;dbl&gt;   &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt;  &lt;fctr&gt;
##  1    8.5        0      0      1      0      0      0      1       1
##  2    8.2        0      0      0      0      0      0      1       2
##  3    8.3        0      0      0      0      0      0      1       3
##  4    8.3        0      0      0      0      0      0      1       4
##  5    8.6        0      0      0      0      0      0      1       5
##  6    8.3        0      0      0      0      0      0      1       6
##  7    9.0        0      0      0      0      0      0      1       7
##  8    8.2        0      0      0      0      0      0      1       8
##  9    8.3        0      0      0      0      0      0      1       9
## 10    8.2        0      0      0      0      0      0      1      10
## # ... with 226 more rows, and 343 more variables: n_ratings &lt;fctr&gt;,
## #   after &lt;fctr&gt;, apothecary &lt;fctr&gt;, armadillo &lt;fctr&gt;, assistant &lt;fctr&gt;,
## #   award &lt;fctr&gt;, baby &lt;fctr&gt;, babysits &lt;fctr&gt;, bag &lt;fctr&gt;, ball &lt;fctr&gt;,
## #   ballroom &lt;fctr&gt;, barbados &lt;fctr&gt;, barry &lt;fctr&gt;, bath &lt;fctr&gt;,
## #   beach &lt;fctr&gt;, bed &lt;fctr&gt;, bing &lt;fctr&gt;, birth &lt;fctr&gt;, birthday &lt;fctr&gt;,
## #   birthing &lt;fctr&gt;, blackout &lt;fctr&gt;, blind &lt;fctr&gt;, boob &lt;fctr&gt;,
## #   boobies &lt;fctr&gt;, book &lt;fctr&gt;, boots &lt;fctr&gt;, box &lt;fctr&gt;, brain &lt;fctr&gt;,
## #   breast &lt;fctr&gt;, bullies &lt;fctr&gt;, bus &lt;fctr&gt;, butt &lt;fctr&gt;,
## #   c.h.e.e.s.e &lt;fctr&gt;, cake &lt;fctr&gt;, candy &lt;fctr&gt;, car &lt;fctr&gt;, cat &lt;fctr&gt;,
## #   caught &lt;fctr&gt;, champion &lt;fctr&gt;, cheap &lt;fctr&gt;, cheesecakes &lt;fctr&gt;,
## #   chick &lt;fctr&gt;, chicken &lt;fctr&gt;, christmas &lt;fctr&gt;, class &lt;fctr&gt;,
## #   closet &lt;fctr&gt;, consuela &lt;fctr&gt;, cookies &lt;fctr&gt;, cooking &lt;fctr&gt;,
## #   cop &lt;fctr&gt;, cousin &lt;fctr&gt;, cries &lt;fctr&gt;, crosses &lt;fctr&gt;, crush &lt;fctr&gt;,
## #   cry &lt;fctr&gt;, cuffs &lt;fctr&gt;, dad &lt;fctr&gt;, dancing &lt;fctr&gt;, date &lt;fctr&gt;,
## #   dates &lt;fctr&gt;, day &lt;fctr&gt;, denial &lt;fctr&gt;, detergent &lt;fctr&gt;,
## #   device &lt;fctr&gt;, dies &lt;fctr&gt;, dinner &lt;fctr&gt;, dirty &lt;fctr&gt;, dogs &lt;fctr&gt;,
## #   dollhouse &lt;fctr&gt;, donor &lt;fctr&gt;, dozen &lt;fctr&gt;, dr &lt;fctr&gt;, dream &lt;fctr&gt;,
## #   dress &lt;fctr&gt;, dresses &lt;fctr&gt;, duck &lt;fctr&gt;, east &lt;fctr&gt;, eddie &lt;fctr&gt;,
## #   eggplant &lt;fctr&gt;, elizabeth &lt;fctr&gt;, embryos &lt;fctr&gt;, emma &lt;fctr&gt;,
## #   engagement &lt;fctr&gt;, estelle &lt;fctr&gt;, everybody &lt;fctr&gt;, evil &lt;fctr&gt;,
## #   factor &lt;fctr&gt;, fake &lt;fctr&gt;, fantasy &lt;fctr&gt;, fertility &lt;fctr&gt;,
## #   fighting &lt;fctr&gt;, fine &lt;fctr&gt;, flashback &lt;fctr&gt;, flirt &lt;fctr&gt;,
## #   football &lt;fctr&gt;, forward &lt;fctr&gt;, frank &lt;fctr&gt;, free &lt;fctr&gt;,
## #   french &lt;fctr&gt;, fridge &lt;fctr&gt;, ...
# also created `ep_mm`, a sparse model matrix based on `episodes`</code></pre>
<p>This is a dataset with a million factors for different words that show up in titles, and factors for all the directors and writers. There are only 236 episodes/observations but 351 potential explanatory variables, leaving us in a high-dimensional situation <code>p&gt;&gt;n</code> (although a pretty low dimensional high dimensional situation).</p>
</div>
<div id="do-naive-linear-model-ols" class="section level3">
<h3>Do naive linear model / OLS</h3>
<p>The naive linear model resolves the high dimensional problem by selecting variables manually: I want to control for season and episode effects, and estimate the character effects.</p>
<pre class="r"><code>naive &lt;- lm(rating ~ season + episode + 
              chandler + joey + monica + 
              phoebe + rachel + ross, 
            data = episodes)</code></pre>
</div>
<div id="do-naive-lasso" class="section level3">
<h3>Do naive LASSO</h3>
<p>Or we could use LASSO to select our control variables for us; we remove the penalty on the characters to ensure these coefficients are estimated to be non-zero.</p>
<pre class="r"><code># penalty factor: 0 for the first 6 variables (the characters), 
# then 1 for the rest.
penalty.factor &lt;- c(rep(0, 6), rep(1, ncol(ep_mm) - 6))

# use `glmnet` to run cross-validated lasso with new penalty.factor
c &lt;- cv.glmnet(ep_mm, episodes$rating, penalty.factor = penalty.factor) %&gt;%
  coef(s = &#39;lambda.min&#39;) %&gt;% .[-1,] # save the optimal coefficients 

# then create dataset of coefficient estimates
lasso &lt;- tibble(term = names(c), estimate = c)
lasso
## # A tibble: 236 x 2
##         term      estimate
##        &lt;chr&gt;         &lt;dbl&gt;
##  1 chandler1  0.1176019972
##  2     joey1 -0.0471344876
##  3   monica1 -0.0344661784
##  4   phoebe1 -0.1441493845
##  5   rachel1 -0.0431721298
##  6     ross1  0.1990786702
##  7   season2  0.0000000000
##  8   season3 -0.0001827438
##  9   season4  0.0006538244
## 10   season5  0.0597535505
## # ... with 226 more rows</code></pre>
</div>
<div id="do-post-selection-estimation" class="section level3">
<h3>Do post-selection estimation</h3>
<pre class="r"><code>
get_lasso_coefs &lt;- function(name, x) { 
# function that returns non-zero coefficients that
# that predict one of the RHS variables
  c &lt;- cv.glmnet(x[, colnames(x) != name], x[,name]) %&gt;%
    coef(s = &#39;lambda.min&#39;) %&gt;% .[-1,]
  names(c)[c != 0]
}

get_lasso_coefs_ &lt;- function(y, names, x) { 
# function that returns non-zero coefficients that
# that predict the RHS variables (first removing the characters)
  c &lt;- cv.glmnet(x[, !colnames(x) %in% names], y) %&gt;%
    coef(s = &#39;lambda.min&#39;) %&gt;% .[-1,]
  names(c)[c != 0] # only return names that are non-zero
}</code></pre>
<p>With those functions, now we loop over the relevant character variables and return non-zero LASSO coefficients.</p>
<pre class="r"><code># list of characters
friends &lt;- c(&quot;chandler&quot;, 
             &quot;joey&quot;, 
             &quot;monica&quot;, 
             &quot;phoebe&quot;, 
             &quot;rachel&quot;, 
             &quot;ross&quot;) %&gt;% 
  map_chr(paste0, &quot;1&quot;)

# map list of characters into lasso coefficient function
lasso_coefs &lt;- friends %&gt;%
  map(get_lasso_coefs, x = ep_mm) %&gt;% 
  unlist() 

# put all the selected variables together
post_vars &lt;- c(friends, 
          lasso_coefs,
          get_lasso_coefs_(episodes$rating, friends, ep_mm)) %&gt;% unique()

# process the variables: &#39;episode1&#39; should be &#39;episode1&#39;, 
# but &#39;chandler1&#39; should be &#39;chandler&#39;
post_vars &lt;- post_vars %&gt;% 
  as_tibble() %&gt;% 
  mutate(value = ifelse(grepl(&quot;season|episode&quot;, value), 
                        value, 
                        gsub(&quot;\\d$&quot;, &quot;&quot;, value))) %&gt;%
  pull(value) %&gt;% c(gsub(&quot;1&quot;, &quot;&quot;, friends), .) %&gt;% unique()

post_dat &lt;- episodes %&gt;% mutate(x = 1) %&gt;% 
  spread(key = season, value = x, fill = 0, sep = &quot;&quot;) %&gt;% mutate(x = 1) %&gt;% 
  spread(key = episode, value = x, fill = 0, sep = &quot;&quot;) %&gt;% 
  mutate_at(vars(-rating), factor) %&gt;% 
  select(one_of(c(&quot;rating&quot;, post_vars)))

# the post-selection lm formula
f &lt;- paste(&quot;rating ~ &quot;, paste(post_vars, collapse = &quot; + &quot;))</code></pre>
</div>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<pre class="r"><code>coefs &lt;- bind_rows(
  # post-selection model coefficients
  lm(f, data = post_dat) %&gt;% summary() %&gt;% tidy() %&gt;% 
    .[2:7,] %&gt;% as_tibble() %&gt;% mutate(model = &quot;post lm&quot;),
  # naive lm model coefficients
  naive %&gt;% tidy() %&gt;% tail() %&gt;% as_tibble() %&gt;% mutate(model = &quot;naive lm&quot;),
  # lasso selection
  lasso[1:6, ] %&gt;%  mutate(model = &quot;lasso&quot;)
)

# bar chart for coefficients + errorbars by character and model
coefs %&gt;% 
  ggplot(aes(x = factor(term), y = estimate, group = factor(model), fill = factor(model))) + 
  geom_col(position = &#39;dodge&#39;) + 
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
    width = 0.4,
    position = position_dodge(width = 0.9)
  ) + labs(title = &#39;Post-selection model coefficients are the best&#39;, x = &#39;Character&#39;, y = &#39;Estimate&#39;)
## Warning: Removed 6 rows containing missing values (geom_errorbar).</code></pre>
<p><img src="/post/2017-12-22-post-selection-inference-on-friends-titles-in-r_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The naive linear model (significantly) says Joey sucks! If Joey‚Äôs in an episode, the episode is typically 0.19 stars lower! Phoebe also sucks, but not with such certainty. The naive linear model is also pretty sure none of the other characters matter.</p>
<p>The naive LASSO model reverses that: Joey doesn‚Äôt matter as much; but Phoebe still sucks and Ross is now great! (NB: you know that‚Äôs not true, Ross is trash.)</p>
<p>Ok, I was wrong, the post-selection procedure thinks Ross is great. Most of the other characters‚Äô estimates are closer to zero, and much different than the naive linear model. Phoebe looks especially vindicated by the post-selection model. (The error bars really drive home the uncertainty, thanks to the tips in the <a href="http://ggplot2.tidyverse.org/reference/position_dodge.html">ggplot2 docs</a> I found while trying to figure out <code>position = 'dodge'</code> in the <code>goem_col</code>s.) Of course we‚Äôre not dealing with a sample of Friends eps, so it‚Äôs hard to say what the standard errors are supposd to represent, really. But that‚Äôs a subject for another time.</p>
<p>What does that mean for us? I thought character names would matter (e.g., ‚ÄòRachel‚Äô episodes would be rated higher), they don‚Äôt, and post-selection inference seems robust. So when I write my Friends episodes, I don‚Äôt have to care too much about putting characters in the titles, other than Ross, and I‚Äôm defo not writing a Ross episode. Or maybe I‚Äôll write ‚ÄúThe One without Ross‚Äù.</p>
</div>
