---
title: "Friends Title Generator, Part 2: parts of speech"
author: "Jesse Tweedle"
date: '2017-12-24'
slug: friends-title-generator-2
description: "This post includes a R code script to generate Friends episode titles, focusing on using parts of speech."
tags: ["r", "friends", "tidytext", "nlp", "markov model"]
categories: ["r", "friends"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(tidyverse)
library(tidytext)
# library(werfriends)
```

We're back on the Friends script grind. 

``` {r}
titles <- werfriends::friends_episodes %>% select(-director, -writers)
titles
```

Approach: instead of calculating word transition probabilities directly from the titles, we're going to use the same approach to generate sentence structures, which will look like "The One Where [Noun] [Verb]". Then, I'll randomly select a noun from all the nouns in the set of titles, and randomly select a verb from all the verbs in the set of titles. That'll give me "The One Where Ross Dies". Perfect.

``` {r}
title_words <- titles %>% 
  unnest_tokens(word, title) %>% # convert titles to words
  group_by(season, episode) %>% 
  mutate(lineno = row_number()) %>% 
  group_by(season, episode, lineno) %>%
  # e.g., split "ross's" into list("ross", "'s"):
  mutate(x = ifelse(grepl("'", word), 
                    str_match(word, "([a-z]*)('s)") %>% .[,-1] %>% lst(),
                    word %>% lst())) %>% 
  unnest() %>% # unnest those lists
  ungroup() %>% 
  # if "ross's", save new rows as "ross" and "'s":
  mutate(word = coalesce(x, word)) %>% 
  select(-x)
```

## Get parts of speech data

`tidytext` comes with the Moby parts of speech dataset. For some reason, this says "a" and "an" are definite articles, not indefinite articles. In addition, they give several options for each word (e.g., "like" could be 6 different parts of speech).

Instead, I found a different [parts of speech dataset](https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/overview.htm) from Ghent University, which I think is included in the [English Lexicon Project](http://elexicon.wustl.edu/). The **SUBTLEXus** dataset is based on parts of speech tagging via [CLAW4](http://ucrel.lancs.ac.uk/claws/) on subtitles in US films and tv shows. I'm not sure about the licensing, but the ELP specifically says it's for non-commercial use only. So don't make any money off this post plz.

``` {r}
# readr unzips stuff! tyvm tidytext documentation.
parts_of_speech <- read_tsv("SUBTLEX-US_frequency_list_with_PoS_information_final_text_version.zip")
parts_of_speech
```

## Apply parts of speech data, and find `pos->pos` transition probabilities

``` {r}
# join parts_of_speech data to our title_words
titles_pos <- title_words %>% 
  left_join(parts_of_speech %>% select(word = Word, pos = Dom_PoS_SUBTLEX)) %>%
  mutate(pos = case_when(
    grepl("'", word) ~ word, # if my word is "'"
    is.na(pos) ~ "Noun",     # if it didn't match, say it's "Noun"
    TRUE ~ pos               # otherwise use what parts_of_speech says 
  )) 
```

What's the most frequent sentence structure?

``` {r}
# what's the most frequent title structure?
titles_pos %>% 
  group_by(season, episode) %>%
  mutate(pos = ifelse(lineno<=3, word, pos)) %>% 
  summarize(title = paste0(word, collapse = " "),
            pos = paste0(pos, collapse = " ")) %>% 
  group_by(pos) %>% count(sort = TRUE)
```

Now we have an idea of typical sentence/title structures. Now all we have to do is calculate the probability that a certain part of speech follows another, then randomly generate a new title structure, and then randomly sample from each part of speech to substitute into the new title structure.

``` {r}
# find the transition probabilities from parts_of_speech to other
# parts_of_speech based on the set of titles we have
pos_transitions <- titles_pos %>% 
  group_by(season, episode) %>%
  filter(lineno >= 2) %>% # only start with "one"
  # and keep "one", "where", "with", "after", etc:
  mutate(pos = ifelse(lineno<=3, word, pos), 
         nxt = lead(pos), 
         nxt = ifelse(is.na(nxt), "EOL", nxt)) %>% # and add the "EOL" character
  group_by(pos, nxt) %>% 
  count() %>% 
  group_by(pos) %>% 
   # calculate the frequency of transitions from `pos` to `nxt` for each `pos`
  mutate(weight = n / sum(n))
pos_transitions
```

This gives a tidy dataset of transition probabilities from one part of speech to another.

## Final step:

Calculate the frequency of each word in each part of speech. E.g., what's the frequency that "rachel" is used relative to all the nouns? We'll use this to replace the parts of speech in a generated sentence structure.

``` {r}
# What's the frequency of each word, by noun.
word_pos_freq <- titles_pos %>% 
  count(word, pos) %>% 
  group_by(pos) %>% 
  mutate(weight = n / sum(n))
word_pos_freq %>% arrange(-n)
```

With that, all we have to do is write a function that generates a title structure using parts of speech, 

``` {r}
# generate sentence structure
generate_structure <- function() {
  # initial title; everything starts with "the one" (except "the last one"s).
  new_title <- c("the", "one")
  
  # while the last word we put in wasn't the end of the title.
  while(new_title[length(new_title)] != "EOL") {
    # add a random word to the current title
    new_title <- c(new_title, 
                   pos_transitions %>% 
                     filter(pos == new_title[length(new_title)]) %>% 
                     sample_n(size = 1, weight = weight) %>% 
                     pull(nxt))
  }
  new_title # return the list of the new title's parts of speech.
}


generate_title <- function() {
  # generate new sentence structure.
  new_title <- generate_structure()

  # use the sentence structure, merge on word-pos frequencies to 
  # sample from.
  new_title %>% enframe() %>% 
    left_join(word_pos_freq, by = c("value" = "pos")) %>% 
    mutate(weight = ifelse(is.na(weight), 1, weight)) %>% 
    group_by(name) %>% 
    # sample a word for each pos
    sample_n(size = 1, weight = weight) %>% 
    mutate(word = ifelse(is.na(word), value, word)) %>% 
    # drop the end
    filter(word != "EOL") %>% 
    ungroup() %>% 
    # then collapse the title back together
    summarize(title = paste(word, collapse = " ")) %>% 
    pull(title) %>% 
    gsub(" 's ", "'s ", .) %>%  # stick the possesive back on its noun
    str_to_title() %>%
    gsub("Pbs", "PBS", .) %>% 
    gsub("C.h.e.e.s.e", "C.H.E.E.S.E.", .)# in case we picked PBS.
}
```

## Now make a pos title, R!

``` {r}
generate_title() 
```

Dang, I hope these are good. But they're generated when Netlify builds the site so `r emo::ji('woman_shrugging')`.

And five more, for fun:
```{r}
replicate(5, generate_title())
```

And one randomly generated header title:

# `r generate_title()`

