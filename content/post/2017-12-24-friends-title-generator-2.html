---
title: "Friends Title Generator, Part 2: parts of speech"
author: "Jesse Tweedle"
date: '2017-12-24'
slug: friends-title-generator-2
description: "This post includes a R code script to generate Friends episode titles, focusing on using parts of speech."
tags: ["r", "friends", "tidytext", "nlp", "markov model"]
categories: ["r", "friends"]
---



<p>We‚Äôre back on the Friends script grind.</p>
<pre class="r"><code>titles &lt;- werfriends::friends_episodes %&gt;% select(-director, -writers)
titles
## # A tibble: 236 x 5
##    season episode                                           title rating
##     &lt;dbl&gt;   &lt;dbl&gt;                                           &lt;chr&gt;  &lt;dbl&gt;
##  1      1       1           The One Where Monica Gets a Roommate¬†    8.5
##  2      1       2           The One with the Sonogram at the End¬†    8.2
##  3      1       3                         The One with the Thumb¬†    8.3
##  4      1       4             The One with George Stephanopoulos¬†    8.3
##  5      1       5 The One with the East German Laundry Detergent¬†    8.6
##  6      1       6                          The One with the Butt¬†    8.3
##  7      1       7                      The One with the Blackout¬†    9.0
##  8      1       8                  The One Where Nana Dies Twice¬†    8.2
##  9      1       9               The One Where Underdog Gets Away¬†    8.3
## 10      1      10                        The One with the Monkey¬†    8.2
## # ... with 226 more rows, and 1 more variables: n_ratings &lt;dbl&gt;</code></pre>
<p>Approach: instead of calculating word transition probabilities directly from the titles, we‚Äôre going to use the same approach to generate sentence structures, which will look like ‚ÄúThe One Where [Noun] [Verb]‚Äù. Then, I‚Äôll randomly select a noun from all the nouns in the set of titles, and randomly select a verb from all the verbs in the set of titles. That‚Äôll give me ‚ÄúThe One Where Ross Dies‚Äù. Perfect.</p>
<pre class="r"><code>title_words &lt;- titles %&gt;% 
  unnest_tokens(word, title) %&gt;% # convert titles to words
  group_by(season, episode) %&gt;% 
  mutate(lineno = row_number()) %&gt;% 
  group_by(season, episode, lineno) %&gt;%
  # e.g., split &quot;ross&#39;s&quot; into list(&quot;ross&quot;, &quot;&#39;s&quot;):
  mutate(x = ifelse(grepl(&quot;&#39;&quot;, word), 
                    str_match(word, &quot;([a-z]*)(&#39;s)&quot;) %&gt;% .[,-1] %&gt;% lst(),
                    word %&gt;% lst())) %&gt;% 
  unnest() %&gt;% # unnest those lists
  ungroup() %&gt;% 
  # if &quot;ross&#39;s&quot;, save new rows as &quot;ross&quot; and &quot;&#39;s&quot;:
  mutate(word = coalesce(x, word)) %&gt;% 
  select(-x)</code></pre>
<div id="get-parts-of-speech-data" class="section level2">
<h2>Get parts of speech data</h2>
<p><code>tidytext</code> comes with the Moby parts of speech dataset. For some reason, this says ‚Äúa‚Äù and ‚Äúan‚Äù are definite articles, not indefinite articles. In addition, they give several options for each word (e.g., ‚Äúlike‚Äù could be 6 different parts of speech).</p>
<p>Instead, I found a different <a href="https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/overview.htm">parts of speech dataset</a> from Ghent University, which I think is included in the <a href="http://elexicon.wustl.edu/">English Lexicon Project</a>. The <strong>SUBTLEXus</strong> dataset is based on parts of speech tagging via <a href="http://ucrel.lancs.ac.uk/claws/">CLAW4</a> on subtitles in US films and tv shows. I‚Äôm not sure about the licensing, but the ELP specifically says it‚Äôs for non-commercial use only. So don‚Äôt make any money off this post plz.</p>
<pre class="r"><code># readr unzips stuff! tyvm tidytext documentation.
parts_of_speech &lt;- read_tsv(&quot;SUBTLEX-US_frequency_list_with_PoS_information_final_text_version.zip&quot;)
## Warning in strptime(x, format, tz = tz): unknown timezone &#39;default/America/
## Toronto&#39;
## Parsed with column specification:
## cols(
##   Word = col_character(),
##   FREQcount = col_integer(),
##   CDcount = col_integer(),
##   FREQlow = col_integer(),
##   Cdlow = col_integer(),
##   SUBTLWF = col_double(),
##   Lg10WF = col_double(),
##   SUBTLCD = col_double(),
##   Lg10CD = col_double(),
##   Dom_PoS_SUBTLEX = col_character(),
##   Freq_dom_PoS_SUBTLEX = col_character(),
##   Percentage_dom_PoS = col_character(),
##   All_PoS_SUBTLEX = col_character(),
##   All_freqs_SUBTLEX = col_character()
## )
parts_of_speech
## # A tibble: 74,286 x 14
##        Word FREQcount CDcount FREQlow Cdlow  SUBTLWF Lg10WF SUBTLCD Lg10CD
##       &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1        a   1041179    8382  976941  8380 20415.27 6.0175   99.93 3.9234
##  2       aa        87      70       6     5     1.71 1.9445    0.83 1.8513
##  3      aaa        25      23       5     3     0.49 1.4150    0.27 1.3802
##  4      aah      2688     634      52    37    52.71 3.4296    7.56 2.8028
##  5    aahed         1       1       1     1     0.02 0.3010    0.01 0.3010
##  6   aahing         2       2       2     2     0.04 0.4771    0.02 0.4771
##  7     aahs         5       4       5     4     0.10 0.7782    0.05 0.6990
##  8      aal         1       1       1     1     0.02 0.3010    0.01 0.3010
##  9 aardvark        21      12      14     8     0.41 1.3424    0.14 1.1139
## 10    aargh        33      26       2     1     0.65 1.5315    0.31 1.4314
## # ... with 74,276 more rows, and 5 more variables: Dom_PoS_SUBTLEX &lt;chr&gt;,
## #   Freq_dom_PoS_SUBTLEX &lt;chr&gt;, Percentage_dom_PoS &lt;chr&gt;,
## #   All_PoS_SUBTLEX &lt;chr&gt;, All_freqs_SUBTLEX &lt;chr&gt;</code></pre>
</div>
<div id="apply-parts-of-speech-data-and-find-pos-pos-transition-probabilities" class="section level2">
<h2>Apply parts of speech data, and find <code>pos-&gt;pos</code> transition probabilities</h2>
<pre class="r"><code># join parts_of_speech data to our title_words
titles_pos &lt;- title_words %&gt;% 
  left_join(parts_of_speech %&gt;% select(word = Word, pos = Dom_PoS_SUBTLEX)) %&gt;%
  mutate(pos = case_when(
    grepl(&quot;&#39;&quot;, word) ~ word, # if my word is &quot;&#39;&quot;
    is.na(pos) ~ &quot;Noun&quot;,     # if it didn&#39;t match, say it&#39;s &quot;Noun&quot;
    TRUE ~ pos               # otherwise use what parts_of_speech says 
  )) 
## Joining, by = &quot;word&quot;</code></pre>
<p>What‚Äôs the most frequent sentence structure?</p>
<pre class="r"><code># what&#39;s the most frequent title structure?
titles_pos %&gt;% 
  group_by(season, episode) %&gt;%
  mutate(pos = ifelse(lineno&lt;=3, word, pos)) %&gt;% 
  summarize(title = paste0(word, collapse = &quot; &quot;),
            pos = paste0(pos, collapse = &quot; &quot;)) %&gt;% 
  group_by(pos) %&gt;% count(sort = TRUE)
## # A tibble: 101 x 2
## # Groups:   pos [101]
##                                     pos     n
##                                   &lt;chr&gt; &lt;int&gt;
##  1            the one with Article Noun    36
##  2       the one with Article Noun Noun    19
##  3            the one with Name &#39;s Noun    18
##  4  the one with Article Adjective Noun    11
##  5 the one with Determiner Article Noun     8
##  6            the one with Noun &#39;s Noun     6
##  7              the one where Noun Verb     5
##  8  the one with Article Noun Noun Noun     5
##  9  the one with Name &#39;s Adjective Noun     5
## 10            the one in Name Noun Noun     4
## # ... with 91 more rows</code></pre>
<p>Now we have an idea of typical sentence/title structures. Now all we have to do is calculate the probability that a certain part of speech follows another, then randomly generate a new title structure, and then randomly sample from each part of speech to substitute into the new title structure.</p>
<pre class="r"><code># find the transition probabilities from parts_of_speech to other
# parts_of_speech based on the set of titles we have
pos_transitions &lt;- titles_pos %&gt;% 
  group_by(season, episode) %&gt;%
  filter(lineno &gt;= 2) %&gt;% # only start with &quot;one&quot;
  # and keep &quot;one&quot;, &quot;where&quot;, &quot;with&quot;, &quot;after&quot;, etc:
  mutate(pos = ifelse(lineno&lt;=3, word, pos), 
         nxt = lead(pos), 
         nxt = ifelse(is.na(nxt), &quot;EOL&quot;, nxt)) %&gt;% # and add the &quot;EOL&quot; character
  group_by(pos, nxt) %&gt;% 
  count() %&gt;% 
  group_by(pos) %&gt;% 
   # calculate the frequency of transitions from `pos` to `nxt` for each `pos`
  mutate(weight = n / sum(n))
pos_transitions
## # A tibble: 119 x 4
## # Groups:   pos [31]
##          pos       nxt     n     weight
##        &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;
##  1        &#39;s Adjective    11 0.21568627
##  2        &#39;s   Article     1 0.01960784
##  3        &#39;s      Noun    36 0.70588235
##  4        &#39;s      Verb     3 0.05882353
##  5 Adjective Adjective     1 0.02857143
##  6 Adjective       EOL     5 0.14285714
##  7 Adjective    Letter     1 0.02857143
##  8 Adjective      Name     1 0.02857143
##  9 Adjective      Noun    22 0.62857143
## 10 Adjective      Verb     5 0.14285714
## # ... with 109 more rows</code></pre>
<p>This gives a tidy dataset of transition probabilities from one part of speech to another.</p>
</div>
<div id="final-step" class="section level2">
<h2>Final step:</h2>
<p>Calculate the frequency of each word in each part of speech. E.g., what‚Äôs the frequency that ‚Äúrachel‚Äù is used relative to all the nouns? We‚Äôll use this to replace the parts of speech in a generated sentence structure.</p>
<pre class="r"><code># What&#39;s the frequency of each word, by noun.
word_pos_freq &lt;- titles_pos %&gt;% 
  count(word, pos) %&gt;% 
  group_by(pos) %&gt;% 
  mutate(weight = n / sum(n))
word_pos_freq %&gt;% arrange(-n)
## # A tibble: 340 x 4
## # Groups:   pos [19]
##      word         pos     n     weight
##     &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;
##  1    the     Article   350 0.97765363
##  2    one      Number   239 0.95983936
##  3   with Preposition   162 0.87096774
##  4  where      Adverb    54 0.78260870
##  5     &#39;s          &#39;s    51 1.00000000
##  6 rachel        Noun    28 0.09395973
##  7   ross        Name    24 0.26966292
##  8   part        Noun    20 0.06711409
##  9   joey        Name    16 0.17977528
## 10    all  Determiner    12 0.80000000
## # ... with 330 more rows</code></pre>
<p>With that, all we have to do is write a function that generates a title structure using parts of speech,</p>
<pre class="r"><code># generate sentence structure
generate_structure &lt;- function() {
  # initial title; everything starts with &quot;the one&quot; (except &quot;the last one&quot;s).
  new_title &lt;- c(&quot;the&quot;, &quot;one&quot;)
  
  # while the last word we put in wasn&#39;t the end of the title.
  while(new_title[length(new_title)] != &quot;EOL&quot;) {
    # add a random word to the current title
    new_title &lt;- c(new_title, 
                   pos_transitions %&gt;% 
                     filter(pos == new_title[length(new_title)]) %&gt;% 
                     sample_n(size = 1, weight = weight) %&gt;% 
                     pull(nxt))
  }
  new_title # return the list of the new title&#39;s parts of speech.
}


generate_title &lt;- function() {
  # generate new sentence structure.
  new_title &lt;- generate_structure()

  # use the sentence structure, merge on word-pos frequencies to 
  # sample from.
  new_title %&gt;% enframe() %&gt;% 
    left_join(word_pos_freq, by = c(&quot;value&quot; = &quot;pos&quot;)) %&gt;% 
    mutate(weight = ifelse(is.na(weight), 1, weight)) %&gt;% 
    group_by(name) %&gt;% 
    # sample a word for each pos
    sample_n(size = 1, weight = weight) %&gt;% 
    mutate(word = ifelse(is.na(word), value, word)) %&gt;% 
    # drop the end
    filter(word != &quot;EOL&quot;) %&gt;% 
    ungroup() %&gt;% 
    # then collapse the title back together
    summarize(title = paste(word, collapse = &quot; &quot;)) %&gt;% 
    pull(title) %&gt;% 
    gsub(&quot; &#39;s &quot;, &quot;&#39;s &quot;, .) %&gt;%  # stick the possesive back on its noun
    str_to_title() %&gt;%
    gsub(&quot;Pbs&quot;, &quot;PBS&quot;, .) %&gt;% 
    gsub(&quot;C.h.e.e.s.e&quot;, &quot;C.H.E.E.S.E.&quot;, .)# in case we picked PBS.
}</code></pre>
</div>
<div id="now-make-a-pos-title-r" class="section level2">
<h2>Now make a pos title, R!</h2>
<pre class="r"><code>generate_title() 
## [1] &quot;The One With Chandler&#39;s Morning&#39;s Party Have&quot;</code></pre>
<p>Dang, I hope these are good. But they‚Äôre generated when Netlify builds the site so ü§∑‚Äç‚ôÄÔ∏è.</p>
<p>And five more, for fun:</p>
<pre class="r"><code>replicate(5, generate_title())
## [1] &quot;The One With The 1&quot;                                                 
## [2] &quot;The One With Armadillo&#39;s Cookies&quot;                                   
## [3] &quot;The One With The Party&quot;                                             
## [4] &quot;The One Where They&#39;re They&#39;re They&#39;re Have Moves Everybody Proposes&quot;
## [5] &quot;The One Where Monica Monkey&quot;</code></pre>
<p>And one randomly generated header title:</p>
</div>
<div id="the-one-with-frank-speaks-the-wedding" class="section level1">
<h1>The One With Frank Speaks The Wedding</h1>
</div>
